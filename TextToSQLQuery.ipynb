# Install and upgrade required packages
!pip install -q transformers flask-cors flask pyngrok torch huggingface_hub cachetools accelerate
!pip install -U bitsandbytes
!pip install --upgrade transformers accelerate

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig
from flask import Flask, request, jsonify
from flask_cors import CORS
from pyngrok import ngrok
import nest_asyncio
from huggingface_hub import login
from cachetools import TTLCache
import gc

# Login to Hugging Face
login("Your token here") # Add your huggingface token here.

# Initialize Flask app
app = Flask(__name__)
CORS(app)

# Initialize cache (cache for 1 hour, max 100 items)
query_cache = TTLCache(maxsize=100, ttl=3600)

# Model configuration
MODEL_ID = "meta-llama/Llama-2-7b-chat-hf"

# Check GPU availability and set device
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Using device: {device}")

# Print CUDA version
if torch.cuda.is_available():
    print(f"CUDA Version: {torch.version.cuda}")

print("Loading model and tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(
    MODEL_ID,
    model_max_length=512,  # Limit context window for tokenizer only
    use_fast=True  # Use fast tokenizer
)

# Configure 4-bit quantization
quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4"
)

# Load model with 4-bit quantization
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    device_map="auto",
    torch_dtype=torch.bfloat16,
    quantization_config=quantization_config,
    low_cpu_mem_usage=True
)

# Create pipeline with optimized settings
generator = pipeline(
    'text-generation',
    model=model,
    tokenizer=tokenizer,
    model_kwargs={"torch_dtype": torch.bfloat16}
)

def clean_gpu_memory():
    """Clean up GPU memory"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
        gc.collect()

def clean_sql_output(text):
    """Extract and clean SQL query from generated text"""
    if "SELECT" in text.upper():
        start = text.upper().find("SELECT")
        text = text[start:]
        end = text.find(';')
        if end != -1:
            text = text[:end+1]
    return text.strip()

def generate_sql_query(text_input):
    try:
        # Check cache first
        cache_key = text_input.strip().lower()
        if cache_key in query_cache:
            return query_cache[cache_key]

        # Clean memory before generation
        clean_gpu_memory()
        
        # More focused prompt with system message
        prompt = f"""[INST] <<SYS>> You are a SQL expert. Generate precise and efficient SQL queries. Keep the queries concise and focused. <</SYS>>
Convert this text to SQL query: {text_input} [/INST]
SELECT"""
        
        # Generate with optimized parameters
        with torch.inference_mode():  # Use inference mode for better performance
            result = generator(
                prompt,
                max_new_tokens=100,
                num_return_sequences=1,
                temperature=0.1,
                do_sample=False,
                pad_token_id=tokenizer.eos_token_id,
                repetition_penalty=1.2,
                num_beams=1,  # Disable beam search for faster generation
                early_stopping=True
            )
        
        # Clean up the generated text
        sql = clean_sql_output(result[0]['generated_text'])
        
        # Cache the result
        query_cache[cache_key] = sql
        
        # Clean memory after generation
        clean_gpu_memory()
            
        return sql

    except Exception as e:
        print(f"Error generating SQL: {str(e)}")
        return "SELECT * FROM table_name;"

@app.route('/generate', methods=['POST'])
def generate():
    try:
        data = request.json
        text_input = data.get('text', '')
        if not text_input:
            return jsonify({'error': 'No text provided'}), 400
        
        print(f"Generating SQL for input: {text_input}")
        query = generate_sql_query(text_input)
        print(f"Generated query: {query}")
        
        return jsonify({'query': query})
        
    except Exception as e:
        print(f"Error in generate endpoint: {str(e)}")
        return jsonify({'error': str(e)}), 500

# Set up ngrok
print("Setting up ngrok...")
!ngrok authtoken "NGROK AUTHTOKEN" # Add your ngrok authtoken.
public_url = ngrok.connect(5000)
print(f"Public URL: {public_url}")

# Enable notebook to run the Flask server
nest_asyncio.apply()

# Print detailed GPU info
print("\nGPU Information:")
print("GPU Available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("Device name:", torch.cuda.get_device_name(0))
    print("Current device:", torch.cuda.current_device())
    print("Memory Allocated:", torch.cuda.memory_allocated(0) / 1024**2, "MB")
    print("Memory Reserved:", torch.cuda.memory_reserved(0) / 1024**2, "MB")

# Run the Flask app
if __name__ == '__main__':
    print("Starting Flask server...")
    app.run(port=5000)
